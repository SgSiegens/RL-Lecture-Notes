@article{greenwade93,
    author  = "George D. Greenwade",
    title   = "The {C}omprehensive {T}ex {A}rchive {N}etwork ({CTAN})",
    year    = "1993",
    journal = "TUGBoat",
    volume  = "14",
    number  = "3",
    pages   = "342--351"
}

@misc{lillicrap2019continuouscontroldeepreinforcement,
      title={Continuous control with deep reinforcement learning}, 
      author={Timothy P. Lillicrap and Jonathan J. Hunt and Alexander Pritzel and Nicolas Heess and Tom Erez and Yuval Tassa and David Silver and Daan Wierstra},
      year={2019},
      eprint={1509.02971},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1509.02971}, 
}

@article{weng2018PG,
  title   = "Policy Gradient Algorithms",
  author  = "Weng, Lilian",
  journal = "lilianweng.github.io",
  year    = "2018",
  url     = "https://lilianweng.github.io/posts/2018-04-08-policy-gradient/"
}

@misc{haarnoja2018softactorcriticoffpolicymaximum,
      title={Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor}, 
      author={Tuomas Haarnoja and Aurick Zhou and Pieter Abbeel and Sergey Levine},
      year={2018},
      eprint={1801.01290},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1801.01290}, 
}

@misc{schulman2017proximalpolicyoptimizationalgorithms,
      title={Proximal Policy Optimization Algorithms}, 
      author={John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
      year={2017},
      eprint={1707.06347},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1707.06347}, 
}

@misc{lynch2019learninglatentplansplay,
      title={Learning Latent Plans from Play}, 
      author={Corey Lynch and Mohi Khansari and Ted Xiao and Vikash Kumar and Jonathan Tompson and Sergey Levine and Pierre Sermanet},
      year={2019},
      eprint={1903.01973},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/1903.01973}, 
}

@misc{kumar2019stabilizingoffpolicyqlearningbootstrapping,
      title={Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction}, 
      author={Aviral Kumar and Justin Fu and George Tucker and Sergey Levine},
      year={2019},
      eprint={1906.00949},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1906.00949}, 
}

@misc{vaswani2023attentionneed,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1706.03762}, 
}

@misc{dai2019transformerxlattentivelanguagemodels,
      title={Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context}, 
      author={Zihang Dai and Zhilin Yang and Yiming Yang and Jaime Carbonell and Quoc V. Le and Ruslan Salakhutdinov},
      year={2019},
      eprint={1901.02860},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1901.02860}, 
}

@misc{parisotto2019stabilizingtransformersreinforcementlearning,
      title={Stabilizing Transformers for Reinforcement Learning}, 
      author={Emilio Parisotto and H. Francis Song and Jack W. Rae and Razvan Pascanu and Caglar Gulcehre and Siddhant M. Jayakumar and Max Jaderberg and Raphael Lopez Kaufman and Aidan Clark and Seb Noury and Matthew M. Botvinick and Nicolas Heess and Raia Hadsell},
      year={2019},
      eprint={1910.06764},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1910.06764}, 
}

@misc{mishra2018simpleneuralattentivemetalearner,
      title={A Simple Neural Attentive Meta-Learner}, 
      author={Nikhil Mishra and Mostafa Rohaninejad and Xi Chen and Pieter Abbeel},
      year={2018},
      eprint={1707.03141},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/1707.03141}, 
}

@misc{chen2021decisiontransformerreinforcementlearning,
      title={Decision Transformer: Reinforcement Learning via Sequence Modeling}, 
      author={Lili Chen and Kevin Lu and Aravind Rajeswaran and Kimin Lee and Aditya Grover and Michael Laskin and Pieter Abbeel and Aravind Srinivas and Igor Mordatch},
      year={2021},
      eprint={2106.01345},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2106.01345}, 
}

@misc{ho2020denoisingdiffusionprobabilisticmodels,
      title={Denoising Diffusion Probabilistic Models}, 
      author={Jonathan Ho and Ajay Jain and Pieter Abbeel},
      year={2020},
      eprint={2006.11239},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2006.11239}, 
}

@misc{lipman2023flowmatchinggenerativemodeling,
      title={Flow Matching for Generative Modeling}, 
      author={Yaron Lipman and Ricky T. Q. Chen and Heli Ben-Hamu and Maximilian Nickel and Matt Le},
      year={2023},
      eprint={2210.02747},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2210.02747}, 
}

@misc{song2021scorebasedgenerativemodelingstochastic,
      title={Score-Based Generative Modeling through Stochastic Differential Equations}, 
      author={Yang Song and Jascha Sohl-Dickstein and Diederik P. Kingma and Abhishek Kumar and Stefano Ermon and Ben Poole},
      year={2021},
      eprint={2011.13456},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2011.13456}, 
}

@misc{bhatt2024crossqbatchnormalizationdeep,
      title={CrossQ: Batch Normalization in Deep Reinforcement Learning for Greater Sample Efficiency and Simplicity}, 
      author={Aditya Bhatt and Daniel Palenicek and Boris Belousov and Max Argus and Artemij Amiranashvili and Thomas Brox and Jan Peters},
      year={2024},
      eprint={1902.05605},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1902.05605}, 
}

@misc{luo2022understandingdiffusionmodelsunified,
      title={Understanding Diffusion Models: A Unified Perspective}, 
      author={Calvin Luo},
      year={2022},
      eprint={2208.11970},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2208.11970}, 
}

@misc{vanhasselt2015deepreinforcementlearningdouble,
      title={Deep Reinforcement Learning with Double Q-learning}, 
      author={Hado van Hasselt and Arthur Guez and David Silver},
      year={2015},
      eprint={1509.06461},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1509.06461}, 
}

@misc{fujimoto2018addressingfunctionapproximationerror,
      title={Addressing Function Approximation Error in Actor-Critic Methods}, 
      author={Scott Fujimoto and Herke van Hoof and David Meger},
      year={2018},
      eprint={1802.09477},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/1802.09477}, 
}

@inproceedings{10.5555/3044805.3044850,
author = {Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
title = {Deterministic policy gradient algorithms},
year = {2014},
publisher = {JMLR.org},
abstract = {In this paper we consider deterministic policy gradient algorithms for reinforcement learning with continuous actions. The deterministic policy gradient has a particularly appealing form: it is the expected gradient of the action-value function. This simple form means that the deterministic policy gradient can be estimated much more efficiently than the usual stochastic policy gradient. To ensure adequate exploration, we introduce an off-policy actor-critic algorithm that learns a deterministic target policy from an exploratory behaviour policy. We demonstrate that deterministic policy gradient algorithms can significantly outperform their stochastic counterparts in high-dimensional action spaces.},
booktitle = {Proceedings of the 31st International Conference on International Conference on Machine Learning - Volume 32},
pages = {I–387–I–395},
location = {Beijing, China},
series = {ICML'14}
}

@misc{schulman2017trustregionpolicyoptimization,
      title={Trust Region Policy Optimization}, 
      author={John Schulman and Sergey Levine and Philipp Moritz and Michael I. Jordan and Pieter Abbeel},
      year={2017},
      eprint={1502.05477},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1502.05477}, 
}


@misc{nauman2024biggerregularizedoptimisticscaling,
      title={Bigger, Regularized, Optimistic: scaling for compute and sample-efficient continuous control}, 
      author={Michal Nauman and Mateusz Ostaszewski and Krzysztof Jankowski and Piotr Miłoś and Marek Cygan},
      year={2024},
      eprint={2405.16158},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.16158}, 
}

@misc{levine2018reinforcementlearningcontrolprobabilistic,
      title={Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review}, 
      author={Sergey Levine},
      year={2018},
      eprint={1805.00909},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1805.00909}, 
}

@article{Blei_2017,
   title={Variational Inference: A Review for Statisticians},
   volume={112},
   ISSN={1537-274X},
   url={http://dx.doi.org/10.1080/01621459.2017.1285773},
   DOI={10.1080/01621459.2017.1285773},
   number={518},
   journal={Journal of the American Statistical Association},
   publisher={Informa UK Limited},
   author={Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
   year={2017},
   month=apr, pages={859–877} }

@misc{finn2016guidedcostlearningdeep,
      title={Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization}, 
      author={Chelsea Finn and Sergey Levine and Pieter Abbeel},
      year={2016},
      eprint={1603.00448},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1603.00448}, 
}

@misc{finn2016connectiongenerativeadversarialnetworks,
      title={A Connection between Generative Adversarial Networks, Inverse Reinforcement Learning, and Energy-Based Models}, 
      author={Chelsea Finn and Paul Christiano and Pieter Abbeel and Sergey Levine},
      year={2016},
      eprint={1611.03852},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1611.03852}, 
}

@misc{brown2019extrapolatingsuboptimaldemonstrationsinverse,
      title={Extrapolating Beyond Suboptimal Demonstrations via Inverse Reinforcement Learning from Observations}, 
      author={Daniel S. Brown and Wonjoon Goo and Prabhat Nagarajan and Scott Niekum},
      year={2019},
      eprint={1904.06387},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1904.06387}, 
}

@misc{levine2020offlinereinforcementlearningtutorial,
      title={Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems}, 
      author={Sergey Levine and Aviral Kumar and George Tucker and Justin Fu},
      year={2020},
      eprint={2005.01643},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2005.01643}, 
}

@misc{OfflineRL_Or_Imitation,
    author= {Aviral Kumar, Ilya Kostrikov, Sergey Levine},
    year  = {Apr 25, 2022},
    title = {Should I Use Offline RL or Imitation Learning?},
    note  = {\url{https://bair.berkeley.edu/blog/2022/04/25/rl-or-bc/}, 
             Last accessed on 2025-04-18},
}

@misc{kumar2020conservativeqlearningofflinereinforcement,
      title={Conservative Q-Learning for Offline Reinforcement Learning}, 
      author={Aviral Kumar and Aurick Zhou and George Tucker and Sergey Levine},
      year={2020},
      eprint={2006.04779},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2006.04779}, 
}

@misc{kostrikov2021offlinereinforcementlearningimplicit,
      title={Offline Reinforcement Learning with Implicit Q-Learning}, 
      author={Ilya Kostrikov and Ashvin Nair and Sergey Levine},
      year={2021},
      eprint={2110.06169},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2110.06169}, 
}

@misc{mnih2013playingatarideepreinforcement,
      title={Playing Atari with Deep Reinforcement Learning}, 
      author={Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Alex Graves and Ioannis Antonoglou and Daan Wierstra and Martin Riedmiller},
      year={2013},
      eprint={1312.5602},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1312.5602}, 
}

@misc{CS285LevineYoutube,
  author = {RAIL},
  year = {2020},
  title = {Deep Reinforcement Learning: CS 285 Fall 2020 - Youtube},
  note = {\url{https://www.youtube.com/playlist?list=PL_iWQOsE6TfURIIhCrlt-wj9ByIVpbfGc}}
}

@misc{CS285,
  author = {Sergey Levine and Kyle Stachowicz and  Vivek Myers and Joey Hong and Kevin Black},
  year = {2020},
  title = {CS 285 at UC Berkeley, Deep Reinforcement Learning},
  note= {\url{https://rail.eecs.berkeley.edu/deeprlcourse/}}
}

@misc{likelihood_ratio_gradient,
  author = { Tim Vieira},
  year = {APR 20, 2019},
  title = {The likelihood-ratio gradient},
  note= {\url{https://timvieira.github.io/blog/post/2019/04/20/the-likelihood-ratio-gradient/}}
}

@misc{OpenAI_Spinning_UP,
  author = {Josh Achiam},
  year = { 2018},
  title = {OpenAI Spinning UP},
  note= {\url{https://spinningup.openai.com/en/latest/index.html}}
}

@misc{Lil'Log_PG,
  author = {Lilian Weng},
  year = {April 8, 2018},
  title = {Policy Gradient Algorithms},
  note= {\url{https://lilianweng.github.io/posts/2018-04-08-policy-gradient/}}
}

@misc{CrossQ_Talk,
  author = {Machine Learning and AI Academy},
  year = {February 2, 2024 },
  title = {AI - Deep Reinforcement learning made easy again! - CrossQ},
  note= {\url{https://www.youtube.com/watch?v=UpACdOz6ntw}}
}

@misc{BRO_Website,
  author = {Michal Nauman and Mateusz Ostaszewski and Krzysztof Jankowski and Piotr Miłoś and Marek Cygan},
  year = {2024},
  title = {Bigger, Regularized, Optimistic},
  note= {\url{https://sites.google.com/view/bro-agent/introduction}}
}

@MISC {Proof_Lagrangian_Concave,
    TITLE = {Why is the lagrange dual function concave?},
    AUTHOR = {(https://math.stackexchange.com/users/253273/a-%ce%93)},
    HOWPUBLISHED = {Mathematics Stack Exchange},
    NOTE = {URL:https://math.stackexchange.com/q/1374410 (version: 2021-04-15)},
    EPRINT = {https://math.stackexchange.com/q/1374410},
    URL = {https://math.stackexchange.com/q/1374410}
}

@misc{agrawal2019differentiableconvexoptimizationlayers,
      title={Differentiable Convex Optimization Layers}, 
      author={Akshay Agrawal and Brandon Amos and Shane Barratt and Stephen Boyd and Steven Diamond and Zico Kolter},
      year={2019},
      eprint={1910.12430},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1910.12430}, 
}

@misc{otto2021differentiabletrustregionlayers,
      title={Differentiable Trust Region Layers for Deep Reinforcement Learning}, 
      author={Fabian Otto and Philipp Becker and Ngo Anh Vien and Hanna Carolin Ziesche and Gerhard Neumann},
      year={2021},
      eprint={2101.09207},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2101.09207}, 
}

@misc{Lagrangian_Duality_for_Dummies,
  author = {David Knowles},
  year = {November 13, 2010},
  title = {Lagrangian Duality for Dummies},
  note= {\url{https://cs.stanford.edu/people/davidknowles/lagrangian_duality.pdf}}
}

@misc{natural_gradients,
  author = {Cody Marie Wild},
  year = {Mar 9, 2019},
  title = {It’s Only Natural: An Excessively Deep Dive Into Natural Gradient Optimization},
  note= {\url{https://medium.com/data-science/its-only-natural-an-excessively-deep-dive-into-natural-gradient-optimization-75d464b89dbb}}
}

@article{JMLR:v25:22-0564,
  author  = {Maximilian H{{\"u}}ttenrauch and Gerhard Neumann},
  title   = {Robust Black-Box Optimization for Stochastic Search and Episodic Reinforcement Learning},
  journal = {Journal of Machine Learning Research},
  year    = {2024},
  volume  = {25},
  number  = {153},
  pages   = {1--44},
  url     = {http://jmlr.org/papers/v25/22-0564.html}
}

@misc{weng2019ES,
  title   = {Evolution Strategies},
  author  = {Weng, Lilian},
  year    = {2019},
  note     = {\url{https://lilianweng.github.io/posts/2019-09-05-evolution-strategies/}}
}

@misc{alphaGo,
  title   = {AlphaGo: How it works technically?},
  author  = {Jonathan Hui},
  year    = {May 13, 2018},
  note     = {\url{https://jonathan-hui.medium.com/alphago-how-it-works-technically-26ddcc085319}}
}

@misc{alphaZero,
  title   = {AlphaZero A step-by-step look at Alpha Zero and Monte Carlo Tree Search},
  author  = {JOSH VARTY},
  year    = {2020},
  note     = {\url{http://joshvarty.github.io/AlphaZero/}}
}


@misc{muZero,
  title   = {MuZero: Mastering Go, chess, shogi and Atari without rules},
  author  = {Julian Schrittwieser and Ioannis Antonoglou and Thomas Hubert and Karen Simonyan and Laurent Sifre and
             Simon Schmitt and Arthur Guez and Edward Lockhart and Demis Hassabis and Thore Graepel and Timothy 
             Lillicrap and David Silver},
  year    = {23 DECEMBER 2020},
  note     = {\url{https://deepmind.google/discover/blog/muzero-mastering-go-chess-shogi-and-atari-without-rules/}}
}

@misc{ho2016generativeadversarialimitationlearning,
      title={Generative Adversarial Imitation Learning}, 
      author={Jonathan Ho and Stefano Ermon},
      year={2016},
      eprint={1606.03476},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1606.03476}, 
}

@misc{CS234Stanford,
  title   = {Stanford CS234 Reinforcement Learning Spring 2024 Emma Brunskill},
  author  = {Stanford Online},
  year    = {2024},
  note     = {\url{https://www.youtube.com/playlistlist=PLoROMvodv4rN4wG6Nk6sNpTEbuOSosZdX}}
}

@misc{DeepRlBootcamp,
  title   = {Deep RL Bootcamp at Berkeley},
  author  = {AI Prism},
  year    = {2018},
  note     = {\url{https://www.youtube.com/@aiprism1155/videos}}
}

@misc{DeepRLandControl,
  title   = {Deep Reinforcement Learning and ControlSpring 2019, CMU 10403},
  author  = {Katerina Fragkiadaki},
  year    = {2019},
  note     = {\url{https://www.andrew.cmu.edu/course/10-403/}}
}

@misc{kumar2022preferofflinereinforcementlearning,
      title={When Should We Prefer Offline Reinforcement Learning Over Behavioral Cloning?}, 
      author={Aviral Kumar and Joey Hong and Anikait Singh and Sergey Levine},
      year={2022},
      eprint={2204.05618},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2204.05618}, 
}

@misc{WordVectors,
  title   = {Word Vectors},
  author  = {Mat Leonard and DanB and Ryan Holbrook and Alexis Cook},
  year    = {2022},
  note     = {\url{https://www.kaggle.com/code/matleonard/word-vectors}}
}

@article{
        poliformer2024,
        author    = {Kuo-Hao Zeng and Zichen Zhang and Kiana Ehsani and Rose Hendrix and Jordi Salvador and Alvaro Herrasti and Ross Girshick and Aniruddha Kembhavi and Luca Weihs},
        title     = {PoliFormer: Scaling On-Policy RL with Transformers Results in Masterful Navigators},
        journal   = {arXiv},
        year      = {2024},
        eprint    = {2406.20083},
}

@misc{weng2018attention,
  title   = {Attention? Attention!},
  author  = {Lilian Weng},
  year    = "2018",
  note     = {\url{https://lilianweng.github.io/posts/2018-06-24-attention/}}
}

@misc{3ble1brownNN,
  title   = {Neural networks},
  author  = {3Blue1Brown},
  year    = "2025",
  note     = {\url{https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi}}
}

@misc{weng2018VAE,
  title   = {From Autoencoder to Beta-VAE},
  author  = {Lilian Weng},
  year    = {2018},
  note     = {\url{https://lilianweng.github.io/posts/2018-08-12-vae/}}
}

@misc{weng2021diffusion,
  title   = {What are Diffusion Models?},
  author  = {Lilian Weng},
  year    = {2021},
  note     = {\url{https://lilianweng.github.io/posts/2021-07-11-diffusion-models/}}
}

@misc{outlier,
  title   = {Youtube Channel: Outlier},
  author  = {Outlier},
  year    = {},
  note     = {\url{https://www.youtube.com/@outliier/videos}}
}

@misc{YangSong,
  title   = {Generative Modeling by Estimating Gradients of the Data Distribution},
  author  = {Yang Song},
  year    = {2021},
  note     = {\url{https://yang-song.net/blog/2021/score/}}
}

 @misc{ wiki:xxx,
   author = "Wikimedia Commons",
   title = "File:Pyplot overfitting.png --- Wikimedia Commons{,} the free media repository",
   year = "2024",
   url = "\url{https://commons.wikimedia.org/w/index.php?title=File:Pyplot_overfitting.png&oldid=892255768}",
   note = "[Online; accessed 27-April-2025]"
 }

@misc{GP,
  title   = {Gaussian Processes regression: basic introductory example},
  author  = {scikit learn},
  year    = {},
  note     = {\url{https://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gpr_noisy_targets.html#sphx-glr-auto-examples-gaussian-process-plot-gpr-noisy-targets-py}}
}

@misc{chua2018deepreinforcementlearninghandful,
      title={Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models}, 
      author={Kurtland Chua and Roberto Calandra and Rowan McAllister and Sergey Levine},
      year={2018},
      eprint={1805.12114},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1805.12114}, 
}

@misc{janner2021trustmodelmodelbasedpolicy,
      title={When to Trust Your Model: Model-Based Policy Optimization}, 
      author={Michael Janner and Justin Fu and Marvin Zhang and Sergey Levine},
      year={2021},
      eprint={1906.08253},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1906.08253}, 
}

@misc{hafner2019learninglatentdynamicsplanning,
      title={Learning Latent Dynamics for Planning from Pixels}, 
      author={Danijar Hafner and Timothy Lillicrap and Ian Fischer and Ruben Villegas and David Ha and Honglak Lee and James Davidson},
      year={2019},
      eprint={1811.04551},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1811.04551}, 
}

@misc{hafner2020dreamcontrollearningbehaviors,
      title={Dream to Control: Learning Behaviors by Latent Imagination}, 
      author={Danijar Hafner and Timothy Lillicrap and Jimmy Ba and Mohammad Norouzi},
      year={2020},
      eprint={1912.01603},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1912.01603}, 
}

@misc{Dreamer_V1,
  title   = {Dreamer V1 Behind our PyTorch implementation of the State-of-the-Art RL algorithm},
  author  = {MICHELE MILESI},
  year    = {16 JUNE 2023},
  note     = {\url{https://eclecticsheep.ai/2023/06/16/dreamer_v1.html}}
}

@misc{danjar,
  title   = {Danijar Hafner  webiste},
  author  = {Danijar Hafner},
  year    = {},
  note     = {\url{https://danijar.com}}
}

@misc{deepmind_ucl,
  title   = {DeepMind x UCL - Deep Learning Lecture Series 2021},
  author  = {Google DeepMind},
  year    = {2021},
  note     = {\url{https://www.youtube.com/playlist?list=PLqYmG7hTraZDVH599EItlEWsUOsJbAodm}}
}

@book{10.5555/3312046, author = {Sutton, Richard S. and Barto, Andrew G.}, title = {Reinforcement Learning: An Introduction}, year = {2018}, isbn = {0262039249}, publisher = {A Bradford Book}, address = {Cambridge, MA, USA}, abstract = {The significantly expanded and updated new edition of a widely used text on reinforcement learning, one of the most active research areas in artificial intelligence. Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms. This second edition has been significantly expanded and updated, presenting new topics and updating coverage of other topics. Like the first edition, this second edition focuses on core online learning algorithms, with the more mathematical material set off in shaded boxes. Part I covers as much of reinforcement learning as possible without going beyond the tabular case for which exact solutions can be found. Many algorithms presented in this part are new to the second edition, including UCB, Expected Sarsa, and Double Learning. Part II extends these ideas to function approximation, with new sections on such topics as artificial neural networks and the Fourier basis, and offers expanded treatment of off-policy learning and policy-gradient methods. Part III has new chapters on reinforcement learning's relationships to psychology and neuroscience, as well as an updated case-studies chapter including AlphaGo and AlphaGo Zero, Atari game playing, and IBM Watson's wagering strategy. The final chapter discusses the future societal impacts of reinforcement learning.} }